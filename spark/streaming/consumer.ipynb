{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea1bd315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vncorenlp\n",
    "# !pip install angdetect\n",
    "# !pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d31f83d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# This funtion will format \"publishedAtDate\" column for later usage\n",
    "def formatToDatetime(date_string):\n",
    "    date_string_without_fraction = date_string[:-5] + 'Z'\n",
    "    date_format = \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "    return datetime.strptime(date_string_without_fraction, date_format)\n",
    "\n",
    "# How to apply to dataframe\n",
    "# df['date_column'] = df['date_column'].apply(lambda x: convert_to_datetime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ddd4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "def removeOtherLanguage(data):\n",
    "    for i in range(len(data)):\n",
    "        try: \n",
    "            if detect(str(data.loc[i, 'text'])) != \"vi\":\n",
    "                #print(str(data.loc[i, 'text']), i)\n",
    "                data = data.drop(i)\n",
    "        except:\n",
    "            #print(str(data.loc[i, 'text']))\n",
    "            data = data.drop(i)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c910126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeReplace(text):\n",
    "    replacements = {\n",
    "        \"òa\": \"oà\", \"óa\": \"oá\", \"ỏa\": \"oả\", \"õa\": \"oã\", \"ọa\": \"oạ\",\n",
    "        \"òe\": \"oè\", \"óe\": \"oé\", \"ỏe\": \"oẻ\", \"õe\": \"oẽ\", \"ọe\": \"oẹ\",\n",
    "        \"ùy\": \"uỳ\", \"úy\": \"uý\", \"ủy\": \"uỷ\", \"ũy\": \"uỹ\", \"ụy\": \"uỵ\",\n",
    "        \"Ủy\": \"Uỷ\", \"\\n\": \".\" , \"\\t\": \".\"  # Add more replacements as needed\n",
    "    }\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "def unicode(data):\n",
    "    data['text'] = data['text'].apply(unicodeReplace)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc3254c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\" \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c51423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base Vietnamese alphabet without tone marks\n",
    "vietnamese_alphabet = \"aăâbcdđeêghiklmnoôơpqrstuưvwxy\"\n",
    "vietnamese_letter_with_tone = \"áàạãảắằẵẳặấầẩẫậéèẻẽẹềếểễệòóỏõọồốổỗộờớởỡợúùũủụứừửữựíìĩỉịýỳỹỷỵ\"\n",
    "\n",
    "# Create uppercase Vietnamese letters with tone marks\n",
    "uppercase_vietnamese_letters_with_tone = [char.upper() for char in vietnamese_letter_with_tone]\n",
    "uppercase_vietnamese_alphabet = vietnamese_alphabet.upper()\n",
    "\n",
    "# Combine the lists into strings\n",
    "lowercase_string = vietnamese_alphabet + \"\".join(vietnamese_letter_with_tone)\n",
    "uppercase_string = uppercase_vietnamese_alphabet + \"\".join(uppercase_vietnamese_letters_with_tone)\n",
    "allcase_string = lowercase_string + uppercase_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d855561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punctuation = \"!\\\"#$%&'()*+,./:;<=>?@[\\]^_`{|}~\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "582b04f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stickyPreprocess(data):\n",
    "    def processText(text):\n",
    "        result = []\n",
    "        for letter_id in range(len(text) - 2):\n",
    "            prev, letter, after = text[letter_id], text[letter_id + 1], text[letter_id + 2]\n",
    "\n",
    "            if letter in punctuation:\n",
    "                if prev in allcase_string:\n",
    "                    result.append(letter_id + 1)\n",
    "                if after in allcase_string:\n",
    "                    result.append(letter_id + 2)\n",
    "\n",
    "            if letter in uppercase_string and prev in lowercase_string and letter_id != 0:\n",
    "                result.extend([letter_id, letter_id + 1])\n",
    "\n",
    "        for index in reversed(result):\n",
    "            text = text[:index] + \" \" + text[index:]\n",
    "\n",
    "        return text\n",
    "\n",
    "    data['text'] = data['text'].apply(processText)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33d07f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "annotator = VnCoreNLP(\"../vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be6a582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "abbreviation_dict = '../vncorenlp/abbreviation_dictionary_vn.xlsx'\n",
    "df = pd.read_csv('../vncorenlp/abbreviation_dictionary_vn.csv')\n",
    "abbreviation_dict = df.set_index(\"abbreviation\")[\"meaning\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5ae17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbreviationPreprocess(data):\n",
    "    def replaceWord(word, dictionary):\n",
    "        return dictionary.get(word, word)\n",
    "\n",
    "    def processText(text):\n",
    "        annotator_text = annotator.tokenize(text)\n",
    "\n",
    "        tokens = [it for sublist in annotator_text for it in sublist if it != '_']\n",
    "        tokens = [replaceWord(it.lower(), abbreviation_dict) for it in tokens]\n",
    "\n",
    "        sentences = [' '.join(sublist) for sublist in annotator_text]\n",
    "\n",
    "        return pd.Series([' '.join(tokens), sentences], index=['text', 'sentences'])\n",
    "\n",
    "    data[['text', 'sentences']] = data['text'].apply(processText)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "624edf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import underthesea\n",
    "\n",
    "def sentimentCal(sentences):\n",
    "    sentiments = [underthesea.sentiment(text) for text in sentences]\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b859ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Ridge from version 1.3.2 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "model = joblib.load('../data_result/model.joblib')\n",
    "selected_columns = [\"pos_prop\", \"neg_prop\", \"reviewLength\", \"reviewHour\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55c3da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pymongo import MongoClient\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6569eae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://dinhhuy:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PREDICT_RATINGS_OF_GOOGLE_LOCAL_REVIEWS_IE212_O11_GROUP7</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2d72fef1bd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scala_version = '2.12'  # your scala version\n",
    "spark_version = '3.5.0' # your spark version\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.6.0' #your kafka version\n",
    "]\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"PREDICT_RATINGS_OF_GOOGLE_LOCAL_REVIEWS_IE212_O11_GROUP7\").config(\"spark.jars.packages\", \",\".join(packages)).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9436ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of KafkaConsumer class\n",
    "bootstrap_servers = 'localhost:9092'\n",
    "topic_name = 'PREDICT_RATINGS_OF_GOOGLE_LOCAL_REVIEWS_IE212_O11_GROUP7'\n",
    "\n",
    "kafkaDf = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", bootstrap_servers)\\\n",
    "                .option(\"subscribe\", topic_name)\\\n",
    "                .option(\"startingOffsets\", \"earliest\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a1a2c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to local server\n",
    "client = MongoClient(\"mongodb://127.0.0.1:27017/\")\n",
    "\n",
    "# Create database called animals\n",
    "mydb = client[\"spark\"]\n",
    "\n",
    "# Create Collection (table) called shelterA\n",
    "collection = mydb.reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32184c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-24 23:38:38,835 https://github.com/undertheseanlp/playground/releases/download/1.3.x/sa_svm_vlsp2016-sa_20210107.zip not found in cache, downloading to C:\\Users\\PC\\AppData\\Local\\Temp\\tmpl0_lsqih\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 2234880/2234880 [00:01<00:00, 1494279.72B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-24 23:38:42,144 copying C:\\Users\\PC\\AppData\\Local\\Temp\\tmpl0_lsqih to cache at C:\\Users\\PC\\.underthesea\\models\\sa_svm_vlsp2016-sa_20210107.zip\n",
      "2024-03-24 23:38:42,158 removing temp file C:\\Users\\PC\\AppData\\Local\\Temp\\tmpl0_lsqih\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reviewId': 'ChZDSUhNMG9nS0VJQ0FnSUR0cVp5OUJ3EAE', 'placeId': 'ChIJ9ZtFSkYvdTERQoHrDARlkUc', 'title': 'Anh Coffee Roastery', 'location/lat': 10.7829311, 'location/lng': 106.6934913, 'categories': 'Quán cà phê, Cửa hàng bánh, Nhà hàng điểm tâm, Cửa hàng bánh ngọt', 'categoryName': 'Quán cà phê', 'reviewerId': '1.11E+20', 'name': 'Tan Nguyen', 'stars': None, 'text': 'quán gì vào ngồi hơn 15p không có 1 nhân_viên nào ra hỏi xem khách uống gì . phục_vụ tệ quá .', 'publishedAtDate': 1707132948000, 'last_update_time': 1711270369.295083, 'sentences': ['Quán gì vào ngồi hơn 15p k có 1 nhân_viên nào ra hỏi xem khách uống gì .', 'Phục_vụ tệ quá'], 'sentiment': ['neutral', 'negative'], 'reviewHour': 11, 'reviewLength': 93, 'num_sentiments': 2, 'pos_prop': 0.0, 'neg_prop': 0.5, 'Predict_rating': 2.8651675571}\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from IPython.display import display, clear_output\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n",
    "\n",
    "# Khởi tạo SparkSession\n",
    "spark = SparkSession.builder.appName(\"PREDICT_RATINGS_OF_GOOGLE_LOCAL_REVIEWS_IE212_O11_GROUP7\").getOrCreate()\n",
    "\n",
    "# Định nghĩa schema cho dữ liệu JSON\n",
    "json_schema = StructType([\n",
    "            StructField(\"reviewId\", StringType(), True),\n",
    "            StructField(\"placeId\", StringType(), True),\n",
    "            StructField(\"title\", StringType(), True),\n",
    "            StructField(\"location/lat\", DoubleType(), True),\n",
    "            StructField(\"location/lng\", DoubleType(), True),\n",
    "            StructField(\"categories\", StringType(), True),\n",
    "            StructField(\"categoryName\", StringType(), True),\n",
    "            StructField(\"reviewerId\", StringType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"stars\", IntegerType(), True),\n",
    "            StructField(\"text\", StringType(), True),\n",
    "            StructField(\"publishedAtDate\", StringType(), True),\n",
    "            StructField(\"last_update_time\", DoubleType(), True)\n",
    "            ])\n",
    "\n",
    "# Đọc dữ liệu từ Kafka topic và chuyển đổi thành DataFrame\n",
    "kafka_stream_df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", bootstrap_servers) \\\n",
    "    .option(\"subscribe\", topic_name) \\\n",
    "    .load()\n",
    "\n",
    "kafkaDf = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", bootstrap_servers)\\\n",
    "                .option(\"subscribe\", topic_name)\\\n",
    "                .option(\"startingOffsets\", \"earliest\").load()\n",
    "\n",
    "# Chuyển đổi giá trị từ JSON string sang struct với schema đã định nghĩa\n",
    "json_stream_df = kafkaDf.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(\"value\", json_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Hàm callback để cập nhật average_rating\n",
    "def update_average_rating(row):\n",
    "    # Kết nối tới MongoDB\n",
    "    client = MongoClient(\"mongodb://127.0.0.1:27017/\")\n",
    "    mydb = client[\"ie212_o11_group7\"]\n",
    "    places_collection = mydb.places\n",
    "    \n",
    "    # Lấy thông tin từ row\n",
    "    place_id = row['placeId']\n",
    "    stars = row['stars'] or 0\n",
    "    predict_rating = row['Predict_rating']\n",
    "    \n",
    "    # Lấy thông tin từ MongoDB\n",
    "    place_info = places_collection.find_one({'placeId': place_id})\n",
    "    \n",
    "    if place_info:\n",
    "        total_rows = place_info['total_rows']\n",
    "\n",
    "        current_average_rating = place_info['average_rating']\n",
    "        new_average_rating = (current_average_rating * total_rows + stars) / (total_rows + 1)\n",
    "\n",
    "        # Tính toán average_rating_new\n",
    "        current_average_predict_rating = place_info['average_predict_rating']\n",
    "        new_average_predict_rating = (current_average_predict_rating * total_rows + predict_rating) / (total_rows + 1)\n",
    "        \n",
    "        # Cập nhật dữ liệu trong MongoDB\n",
    "        places_collection.update_one({'placeId': place_id},\n",
    "                                     {'$set': {'average_rating': new_average_rating}})\n",
    "\n",
    "        # Cập nhật dữ liệu trong MongoDB\n",
    "        places_collection.update_one({'placeId': place_id},\n",
    "                                     {'$set': {'average_predict_rating': new_average_predict_rating,\n",
    "                                               'total_rows': total_rows + 1}})\n",
    "\n",
    "last_consumer_time = 0\n",
    "while True:\n",
    "    try:\n",
    "        # Sắp xếp DataFrame theo thời gian cập nhật giảm dần và chỉ lấy dòng đầu tiên\n",
    "        newest_review_df = json_stream_df.sort(col(\"last_update_time\").desc()).limit(1)\n",
    "        temp_time = newest_review_df.select(\"last_update_time\").first()[0]\n",
    "        if temp_time > last_consumer_time:\n",
    "            last_consumer_time = temp_time\n",
    "        else:\n",
    "            # Nếu thời gian không lớn hơn, tiếp tục vòng lặp\n",
    "            continue\n",
    "\n",
    "        # newest_review_df = json_stream_df.limit(1)\n",
    "        newest_review_df_pandas = newest_review_df.toPandas()\n",
    "\n",
    "        # Check if the pandas DataFrame is empty\n",
    "        if newest_review_df_pandas.empty:\n",
    "            continue\n",
    "        newest_review_df_pandas = removeOtherLanguage(newest_review_df_pandas)\n",
    "        if newest_review_df_pandas.empty:\n",
    "            continue\n",
    "        \n",
    "        newest_review_df_pandas = unicode(newest_review_df_pandas)\n",
    "        newest_review_df_pandas['text'] = newest_review_df_pandas['text'].apply(remove_emojis)\n",
    "        newest_review_df_pandas = stickyPreprocess(newest_review_df_pandas)\n",
    "        newest_review_df_pandas = abbreviationPreprocess(newest_review_df_pandas)\n",
    "        newest_review_df_pandas[\"sentiment\"] = newest_review_df_pandas[\"sentences\"].apply(sentimentCal)\n",
    "\n",
    "        newest_review_df_pandas['sentiment'] = newest_review_df_pandas['sentiment'].apply(lambda sentiments: [\"neutral\" if sentiment is None else sentiment for sentiment in sentiments])\n",
    "\n",
    "        newest_review_df_pandas[\"text\"] = [item + \" .\" for item in newest_review_df_pandas[\"text\"]]\n",
    "        newest_review_df_pandas['publishedAtDate'] = newest_review_df_pandas['publishedAtDate'].apply(lambda x: formatToDatetime(x))\n",
    "        newest_review_df_pandas[\"reviewHour\"] = [item.hour for item in newest_review_df_pandas[\"publishedAtDate\"]]\n",
    "        newest_review_df_pandas[\"reviewLength\"] = [len(item) for item in newest_review_df_pandas[\"text\"]]\n",
    "\n",
    "        count_pos = newest_review_df_pandas['sentiment'].apply(lambda sentiments: sum(sentiment == \"positive\" for sentiment in sentiments))\n",
    "        count_neg = newest_review_df_pandas['sentiment'].apply(lambda sentiments: sum(sentiment == \"negative\" for sentiment in sentiments))\n",
    "\n",
    "        newest_review_df_pandas['num_sentiments'] = newest_review_df_pandas['sentiment'].apply(lambda sentiments: len(sentiments) if sentiments else 0)\n",
    "\n",
    "        newest_review_df_pandas['pos_prop'] = count_pos / newest_review_df_pandas['num_sentiments']\n",
    "        newest_review_df_pandas['neg_prop'] = count_neg / newest_review_df_pandas['num_sentiments']\n",
    "\n",
    "        X = newest_review_df_pandas[selected_columns].copy()\n",
    "        newest_review_df_pandas[\"Predict_rating\"] = model.predict(X)\n",
    "\n",
    "        # newest_review_df = newest_review_df.drop(columns = ['text'], ['pos_prop'], ['neg_prop'], [])\n",
    "        \n",
    "        # Chuyển DataFrame thành JSON string và lấy giá trị cột 'value'\n",
    "        # message_value = newest_review_df.toJSON().first()\n",
    "        json_string = newest_review_df_pandas.to_json(orient='records')\n",
    "        json_list = json.loads(json_string)\n",
    "        first_row = json_list[0]\n",
    "\n",
    "        # In ra giá trị để kiểm tra\n",
    "        #print(message_value)\n",
    "        print(first_row)\n",
    "\n",
    "        # Insert documents (rows) into the database's collection (table)\n",
    "        collection.insert_one(first_row)\n",
    "        \n",
    "        # Gọi hàm callback để cập nhật average_rating\n",
    "        update_average_rating(first_row)\n",
    "        sleep(10)\n",
    "        # clear_output(wait=True)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"break\")\n",
    "        break\n",
    "\n",
    "print(\"Live view ended...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ec4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "newest_review_df.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8e4642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
